{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc429d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 18 unique speakers:\n",
      "\n",
      " 1. Adil Gazder\n",
      " 2. Pranshul Bhatnagar\n",
      " 3. Lewis Hamilton\n",
      " 4. Neha Senthil\n",
      " 5. Joe Rogan\n",
      " 6. Daymond John\n",
      " 7. Mobasserul Haque\n",
      " 8. Aravind Srinivas\n",
      " 9. Mark Cuban\n",
      "10. Paul Hudson\n",
      "11. Lex Fridman\n",
      "12. Michelle Tandler\n",
      "13. James Sinegal\n",
      "14. Peter Thiel\n",
      "15. Andrew Garfield\n",
      "16. Kevin O'Leary\n",
      "17. Bill Gates\n",
      "18. Paul Varga\n",
      "\n",
      "[+] Fetching: Adil Gazder\n",
      "\n",
      "[+] Fetching: Pranshul Bhatnagar\n",
      "\n",
      "[+] Fetching: Lewis Hamilton\n",
      "\n",
      "[+] Fetching: Neha Senthil\n",
      "\n",
      "[+] Fetching: Joe Rogan\n",
      "\n",
      "[+] Fetching: Daymond John\n",
      "\n",
      "[+] Fetching: Mobasserul Haque\n",
      "\n",
      "[+] Fetching: Aravind Srinivas\n",
      "\n",
      "[+] Fetching: Mark Cuban\n",
      "\n",
      "[+] Fetching: Paul Hudson\n",
      "\n",
      "[+] Fetching: Lex Fridman\n",
      "\n",
      "[+] Fetching: Michelle Tandler\n",
      "\n",
      "[+] Fetching: James Sinegal\n",
      "\n",
      "[+] Fetching: Peter Thiel\n",
      "\n",
      "[+] Fetching: Andrew Garfield\n",
      "\n",
      "[+] Fetching: Kevin O'Leary\n",
      "\n",
      "[+] Fetching: Bill Gates\n",
      "\n",
      "[+] Fetching: Paul Varga\n",
      "\n",
      "✅ Done.\n",
      "Profiles saved under: out_speakers\\profiles\n",
      "Summary: out_speakers\\speakers_summary.json\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import json\n",
    "# import time\n",
    "# import unicodedata\n",
    "# from pathlib import Path\n",
    "# import requests\n",
    "\n",
    "# # Optional: load .env if it exists\n",
    "# try:\n",
    "#     import dotenv\n",
    "#     dotenv.load_dotenv()\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "\n",
    "# # ----------- Configuration -----------\n",
    "# input_path = \"speaker.txt\"         # transcript file\n",
    "# out_dir = \"out_speakers\"           # where to save outputs\n",
    "# pages = 1                          # Google pages per speaker (10 results/page)\n",
    "# pause = 1.5                        # seconds between API calls\n",
    "# query_template = '\"{name}\"'        # search pattern\n",
    "# dry_run = False                    # True = skip Google API calls (for testing)\n",
    "# # ------------------------------------\n",
    "\n",
    "\n",
    "# HEADER_RE = re.compile(r\"^\\[\\s*([^\\]]+?)\\s*:\\]\", flags=re.MULTILINE)\n",
    "\n",
    "# def extract_raw_headers(text):\n",
    "#     return HEADER_RE.findall(text)\n",
    "\n",
    "# def normalize_header_to_name(h):\n",
    "#     \"\"\"Normalize bracket headers into canonical speaker names.\"\"\"\n",
    "#     s = unicodedata.normalize(\"NFKC\", h)\n",
    "#     s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "#     tokens = s.split(\" \")\n",
    "\n",
    "#     def has_non_lower(ts):\n",
    "#         return any(not re.fullmatch(r\"[a-z]+\", t) for t in ts)\n",
    "\n",
    "#     # remove trailing all-lowercase action words\n",
    "#     while tokens and re.fullmatch(r\"[a-z]+\", tokens[-1]):\n",
    "#         if has_non_lower(tokens[:-1]):\n",
    "#             tokens.pop()\n",
    "#         else:\n",
    "#             break\n",
    "#     s = \" \".join(tokens).strip()\n",
    "\n",
    "#     s = s.replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "#     s = re.sub(r\"[^0-9A-Za-z\\s\\.\\-\\'&]\", \" \", s)     # drop emojis/symbols\n",
    "#     s = re.sub(r\"[.,:;!?\\u2026\\-–—]+$\", \"\", s).strip()\n",
    "#     s = re.sub(r\"\\s*\\.\\s*\", \".\", s)\n",
    "#     s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "#     return s\n",
    "\n",
    "# def unique_preserve_order(items):\n",
    "#     seen, out = set(), []\n",
    "#     for x in items:\n",
    "#         if x and x not in seen:\n",
    "#             seen.add(x)\n",
    "#             out.append(x)\n",
    "#     return out\n",
    "\n",
    "\n",
    "# # -------- Google search helper --------\n",
    "# GOOGLE_API = \"https://www.googleapis.com/customsearch/v1\"\n",
    "\n",
    "# def google_search_person(person_name, api_key, cx, num_pages=1, pause=1.5, query_template=\"{name}\"):\n",
    "#     texts, links = [], []\n",
    "#     rendered_q = query_template.format(name=person_name).strip()\n",
    "\n",
    "#     for page in range(num_pages):\n",
    "#         start = page * 10 + 1\n",
    "#         params = {\"key\": api_key, \"cx\": cx, \"q\": rendered_q, \"start\": start}\n",
    "#         resp = requests.get(GOOGLE_API, params=params, timeout=20)\n",
    "#         if resp.status_code == 429:\n",
    "#             print(\"Rate limited — sleeping before retry...\")\n",
    "#             time.sleep(3 + page)\n",
    "#             resp = requests.get(GOOGLE_API, params=params, timeout=20)\n",
    "#         resp.raise_for_status()\n",
    "#         data = resp.json()\n",
    "#         items = data.get(\"items\", [])\n",
    "#         if not items:\n",
    "#             break\n",
    "#         for it in items:\n",
    "#             snippet = it.get(\"snippet\") or \"\"\n",
    "#             link = it.get(\"link\") or \"\"\n",
    "#             if snippet and link:\n",
    "#                 texts.append(snippet)\n",
    "#                 links.append(link)\n",
    "#         time.sleep(max(0.0, pause))\n",
    "\n",
    "#     return {\n",
    "#         \"query\": person_name,\n",
    "#         \"rendered_query\": rendered_q,\n",
    "#         \"total_results\": len(links),\n",
    "#         \"texts\": texts,\n",
    "#         \"links\": links,\n",
    "#     }\n",
    "\n",
    "\n",
    "# # ----------- Main pipeline -----------\n",
    "# API_KEY = os.getenv(\"CUSTOM_SEARCH_API_KEY\", \"\").strip()\n",
    "# CX = os.getenv(\"CUSTOM_SEARCH_ENGINE_ID\", \"\").strip()\n",
    "\n",
    "# text = Path(input_path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "# raw_headers = extract_raw_headers(text)\n",
    "# names = [normalize_header_to_name(h) for h in raw_headers if h.strip()]\n",
    "# speakers = unique_preserve_order(names)\n",
    "\n",
    "# out_dir = Path(out_dir)\n",
    "# (out_dir / \"profiles\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# print(f\"✅ Found {len(speakers)} unique speakers:\\n\")\n",
    "# for i, s in enumerate(speakers, 1):\n",
    "#     print(f\"{i:>2}. {s}\")\n",
    "\n",
    "# summary = {\n",
    "#     \"input\": input_path,\n",
    "#     \"total_unique_speakers\": len(speakers),\n",
    "#     \"query_template\": query_template,\n",
    "#     \"pages\": pages,\n",
    "#     \"pause\": pause,\n",
    "#     \"speakers\": [],\n",
    "#     \"generated_at\": time.time(),\n",
    "# }\n",
    "\n",
    "# if dry_run:\n",
    "#     Path(out_dir / \"speakers_summary.json\").write_text(\n",
    "#         json.dumps(summary, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "#     )\n",
    "#     print(f\"\\n--dry-run=True: wrote only {out_dir/'speakers_summary.json'}\")\n",
    "# else:\n",
    "#     if not API_KEY or not CX:\n",
    "#         raise SystemExit(\"❌ Missing CUSTOM_SEARCH_API_KEY or CUSTOM_SEARCH_ENGINE_ID (.env)\")\n",
    "\n",
    "#     for name in speakers:\n",
    "#         print(f\"\\n[+] Fetching: {name}\")\n",
    "#         try:\n",
    "#             info = google_search_person(\n",
    "#                 person_name=name,\n",
    "#                 api_key=API_KEY,\n",
    "#                 cx=CX,\n",
    "#                 num_pages=pages,\n",
    "#                 pause=pause,\n",
    "#                 query_template=query_template,\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             print(f\"   ✗ Error for {name}: {e}\")\n",
    "#             info = {\n",
    "#                 \"query\": name,\n",
    "#                 \"rendered_query\": query_template.format(name=name),\n",
    "#                 \"total_results\": 0,\n",
    "#                 \"texts\": [],\n",
    "#                 \"links\": [],\n",
    "#                 \"error\": str(e),\n",
    "#             }\n",
    "\n",
    "#         slug = re.sub(r\"[^0-9A-Za-z\\-_]+\", \"_\", name).strip(\"_\")\n",
    "#         out_file = out_dir / \"profiles\" / f\"{slug}_info.json\"\n",
    "#         out_file.write_text(json.dumps(info, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "#         summary[\"speakers\"].append({\n",
    "#             \"name\": name,\n",
    "#             \"file\": str(out_file),\n",
    "#             \"total_results\": info.get(\"total_results\", 0),\n",
    "#         })\n",
    "\n",
    "#     Path(out_dir / \"speakers_summary.json\").write_text(\n",
    "#         json.dumps(summary, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "#     )\n",
    "\n",
    "#     print(f\"\\n✅ Done.\\nProfiles saved under: {out_dir/'profiles'}\\nSummary: {out_dir/'speakers_summary.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4583fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Optional: load .env if it exists\n",
    "try:\n",
    "    import dotenv\n",
    "    dotenv.load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ----------- Configuration -----------\n",
    "input_path = \"speaker.txt\"         # transcript file\n",
    "out_dir = \"out_speakers\"           # where to save outputs\n",
    "pages = 1                          # Google pages per speaker (10 results/page)\n",
    "pause = 1.5                        # seconds between API calls\n",
    "query_template = '\"{name}\"'        # search pattern\n",
    "dry_run = False                    # True = skip Google API calls (for testing)\n",
    "# ------------------------------------\n",
    "\n",
    "HEADER_RE = re.compile(r\"^\\[\\s*([^\\]]+?)\\s*:\\]\", flags=re.MULTILINE)\n",
    "\n",
    "def extract_raw_headers(text):\n",
    "    return HEADER_RE.findall(text)\n",
    "\n",
    "def normalize_header_to_name(h):\n",
    "    \"\"\"Normalize bracket headers into canonical speaker names.\"\"\"\n",
    "    s = unicodedata.normalize(\"NFKC\", h)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    tokens = s.split(\" \")\n",
    "\n",
    "    def has_non_lower(ts):\n",
    "        return any(not re.fullmatch(r\"[a-z]+\", t) for t in ts)\n",
    "\n",
    "    while tokens and re.fullmatch(r\"[a-z]+\", tokens[-1]):\n",
    "        if has_non_lower(tokens[:-1]):\n",
    "            tokens.pop()\n",
    "        else:\n",
    "            break\n",
    "    s = \" \".join(tokens).strip()\n",
    "    s = s.replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    s = re.sub(r\"[^0-9A-Za-z\\s\\.\\-\\'&]\", \" \", s)\n",
    "    s = re.sub(r\"[.,:;!?\\u2026\\-–—]+$\", \"\", s).strip()\n",
    "    s = re.sub(r\"\\s*\\.\\s*\", \".\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def unique_preserve_order(items):\n",
    "    seen, out = set(), []\n",
    "    for x in items:\n",
    "        if x and x not in seen:\n",
    "            seen.add(x)\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "# -------- Helper to scrape full page text --------\n",
    "def scrape_link_text(url, max_chars=15000):\n",
    "    \"\"\"Fetch visible text from a webpage using BeautifulSoup.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        # remove scripts/styles\n",
    "        for s in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\", \"aside\"]):\n",
    "            s.decompose()\n",
    "        text = \" \".join(t.strip() for t in soup.stripped_strings)\n",
    "        return text[:max_chars]  # truncate to max_chars to avoid huge JSON\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching content: {e}\"\n",
    "\n",
    "# -------- Google search + scrape helper --------\n",
    "GOOGLE_API = \"https://www.googleapis.com/customsearch/v1\"\n",
    "\n",
    "def google_search_person(person_name, api_key, cx, num_pages=1, pause=1.5, query_template=\"{name}\"):\n",
    "    links, texts = [], []\n",
    "    rendered_q = query_template.format(name=person_name).strip()\n",
    "\n",
    "    for page in range(num_pages):\n",
    "        start = page * 10 + 1\n",
    "        params = {\"key\": api_key, \"cx\": cx, \"q\": rendered_q, \"start\": start}\n",
    "        resp = requests.get(GOOGLE_API, params=params, timeout=20)\n",
    "        if resp.status_code == 429:\n",
    "            print(\"Rate limited — sleeping before retry...\")\n",
    "            time.sleep(3 + page)\n",
    "            resp = requests.get(GOOGLE_API, params=params, timeout=20)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        for it in items:\n",
    "            link = it.get(\"link\") or \"\"\n",
    "            if link:\n",
    "                links.append(link)\n",
    "                text = scrape_link_text(link)\n",
    "                texts.append(text)\n",
    "                time.sleep(max(0.0, pause))  # be polite\n",
    "\n",
    "    return {\n",
    "        \"query\": person_name,\n",
    "        \"rendered_query\": rendered_q,\n",
    "        \"total_results\": len(links),\n",
    "        \"texts\": texts,\n",
    "        \"links\": links,\n",
    "    }\n",
    "\n",
    "# ----------- Main pipeline -----------\n",
    "API_KEY = os.getenv(\"CUSTOM_SEARCH_API_KEY\", \"\").strip()\n",
    "CX = os.getenv(\"CUSTOM_SEARCH_ENGINE_ID\", \"\").strip()\n",
    "\n",
    "text = Path(input_path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "raw_headers = extract_raw_headers(text)\n",
    "names = [normalize_header_to_name(h) for h in raw_headers if h.strip()]\n",
    "speakers = unique_preserve_order(names)\n",
    "\n",
    "out_dir = Path(out_dir)\n",
    "(out_dir / \"profiles\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"✅ Found {len(speakers)} unique speakers:\\n\")\n",
    "for i, s in enumerate(speakers, 1):\n",
    "    print(f\"{i:>2}. {s}\")\n",
    "\n",
    "summary = {\n",
    "    \"input\": input_path,\n",
    "    \"total_unique_speakers\": len(speakers),\n",
    "    \"query_template\": query_template,\n",
    "    \"pages\": pages,\n",
    "    \"pause\": pause,\n",
    "    \"speakers\": [],\n",
    "    \"generated_at\": time.time(),\n",
    "}\n",
    "\n",
    "if dry_run:\n",
    "    Path(out_dir / \"speakers_summary.json\").write_text(\n",
    "        json.dumps(summary, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "    )\n",
    "    print(f\"\\n--dry-run=True: wrote only {out_dir/'speakers_summary.json'}\")\n",
    "else:\n",
    "    if not API_KEY or not CX:\n",
    "        raise SystemExit(\"❌ Missing CUSTOM_SEARCH_API_KEY or CUSTOM_SEARCH_ENGINE_ID (.env)\")\n",
    "\n",
    "    for name in speakers:\n",
    "        print(f\"\\n[+] Fetching: {name}\")\n",
    "        try:\n",
    "            info = google_search_person(\n",
    "                person_name=name,\n",
    "                api_key=API_KEY,\n",
    "                cx=CX,\n",
    "                num_pages=pages,\n",
    "                pause=pause,\n",
    "                query_template=query_template,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"   ✗ Error for {name}: {e}\")\n",
    "            info = {\n",
    "                \"query\": name,\n",
    "                \"rendered_query\": query_template.format(name=name),\n",
    "                \"total_results\": 0,\n",
    "                \"texts\": [],\n",
    "                \"links\": [],\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "\n",
    "        slug = re.sub(r\"[^0-9A-Za-z\\-_]+\", \"_\", name).strip(\"_\")\n",
    "        out_file = out_dir / \"profiles\" / f\"{slug}_info.json\"\n",
    "        out_file.write_text(json.dumps(info, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "        summary[\"speakers\"].append({\n",
    "            \"name\": name,\n",
    "            \"file\": str(out_file),\n",
    "            \"total_results\": info.get(\"total_results\", 0),\n",
    "        })\n",
    "\n",
    "    Path(out_dir / \"speakers_summary.json\").write_text(\n",
    "        json.dumps(summary, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n✅ Done.\\nProfiles saved under: {out_dir/'profiles'}\\nSummary: {out_dir/'speakers_summary.json'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
